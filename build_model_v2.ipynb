{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import random\n",
    "import keras\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from collections import Counter\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if gpu is available, use it\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    device = 'gpu'\n",
    "else:\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR: str = \"output/\"\n",
    "CSV_FILE_PATH: str = \"bio_metadata.csv\"\n",
    "NATIVE_FILE_PATH: str = \"native_bio_metadata.csv\"\n",
    "NON_NATIVE_FILE_PATH: str = \"non_native_bio_metadata.csv\"\n",
    "NATIVE_LANGUAGES: list[str] = ['uk', 'usa', 'canada']\n",
    "NON_NATIVE_LANGUAGES: list[str] = [\n",
    "    'australia',\n",
    "    'new zealand',\n",
    "    'ireland',    \n",
    "    'singapore',  \n",
    "    'south',     \n",
    "    'africa',   \n",
    "    'jamaica',    \n",
    "    'scotland',   \n",
    "    'islands',\n",
    "]\n",
    "DATASET_DIR: str = \"data/\"\n",
    "NATIVE_DIR: str = \"data/native/\"\n",
    "NATIVE_COMBINED_DIR: str = \"data/native_combined/\"\n",
    "NON_NATIVE_DIR: str = \"data/non_native/\"\n",
    "AUDIO_DATA_DIR: str = \"data/audio/\"\n",
    "AUDIO_FILE_PATH: str = \"data/audio/{}.wav\"\n",
    "SILENCE_THRESHOLD: float = .01\n",
    "RATE: int = 2400\n",
    "N_MFCC: int = 13\n",
    "COL_SIZE: int = 30\n",
    "EPOCHS_SINGLE: int = 150 #35 #50 #250 #100 #500\n",
    "EPOCHS_HYBRID = 20\n",
    "EPOCHS_MULTI = 120\n",
    "LEARNING_RATE = 0.001\n",
    "WAIT: float = 1.2\n",
    "DEBUG: bool = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XFeatureExtractor:\n",
    "    \"\"\"\n",
    "        Extracts acoustic and prosodic features from audio files.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_acoustic_features(file_path, features=('mfcc', 'chroma_stft', 'spectral_centroid')):\n",
    "        try:\n",
    "            y, sr = librosa.load(f'data/audio/{file_path}.wav')\n",
    "            features_dict = {}\n",
    "            for feature_name in features:\n",
    "                if feature_name == 'mfcc':\n",
    "                    features_dict['mfcc'] = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n",
    "                elif feature_name == 'chroma_stft':\n",
    "                    features_dict['chroma_stft'] = librosa.feature.chroma_stft(y=y, sr=sr, n_chroma=12)\n",
    "                elif feature_name == 'spectral_centroid':\n",
    "                    features_dict['spectral_centroid'] = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "                elif feature_name == 'spectral_bandwidth':\n",
    "                    features_dict['spectral_bandwidth'] = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n",
    "                elif feature_name == 'zero_crossing_rate':\n",
    "                    features_dict['zero_crossing_rate'] = librosa.feature.zero_crossing_rate(\n",
    "                        y=y, frame_length=2048, hop_length=512)\n",
    "                elif feature_name == 'rmse':\n",
    "                    features_dict['rmse'] = librosa.feature.rms(y=y)\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported feature: {feature_name}\")\n",
    "\n",
    "            return features_dict\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading file {file_path}: {e}\")\n",
    "            return None  # Or handle error differently\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_prosodic_features(file_path, features=('pitch', 'intensity', 'duration')):\n",
    "        try:\n",
    "            y, sr = librosa.load(f'data/audio/{file_path}.wav')\n",
    "            prosodic_features = {}\n",
    "            for feature_name in features:\n",
    "                if feature_name == 'pitch':\n",
    "                    prosodic_features['pitch'] = librosa.yin(y=y, fmin=65, fmax=2093)\n",
    "                elif feature_name == 'intensity':\n",
    "                    prosodic_features['intensity'] = librosa.feature.rms(y=y)\n",
    "                elif feature_name == 'duration':\n",
    "                    prosodic_features['duration'] = len(y) / sr\n",
    "                elif feature_name == 'formants':\n",
    "                    # Implement formant extraction using librosa or a custom solution\n",
    "                    prosodic_features['formants'] = None  # Placeholder for now\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported feature: {feature_name}\")\n",
    "            return prosodic_features\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading file {file_path}: {e}\")\n",
    "            return None  # Handle loading errors\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_plp(file_path):\n",
    "        y, sr = librosa.load(f'data/audio/{file_path}.wav')\n",
    "        plp = librosa.beat.plp(y=y, sr=sr)\n",
    "        return plp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractorUtils:\n",
    "    \"\"\"\n",
    "        Extracts acoustic features from audio files and preprocesses them for language identification.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass  # No initialization needed for now\n",
    "    \n",
    "    @staticmethod\n",
    "    def to_categorical(y):\n",
    "        '''\n",
    "            Converts list of languages into a binary class matrix\n",
    "            :param y (list): list of languages\n",
    "            :return (numpy array): binary class matrix\n",
    "        '''\n",
    "        lang_dict = {}\n",
    "        for index, language in enumerate(set(y)):\n",
    "            lang_dict[language] = index\n",
    "        y = list(map(lambda x: lang_dict[x],y))\n",
    "        return keras.utils.to_categorical(y, len(lang_dict)), lang_dict\n",
    "\n",
    "    @staticmethod\n",
    "    def get_wav(filename):\n",
    "        \"\"\"\n",
    "            Loads a wav file from disk and resamples to a target sample rate.\n",
    "\n",
    "            Args:\n",
    "                filename (str): Path to the wav file.\n",
    "\n",
    "            Returns:\n",
    "                numpy.ndarray: Down-sampled wav file (or None if an error occurs).\n",
    "        \"\"\"\n",
    "        try:\n",
    "            y, sr = librosa.load(f'./data/native_combined/{filename}.wav')\n",
    "            return librosa.core.resample(y=y, orig_sr=sr, target_sr=RATE, scale=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading wav: {filename} - {e}\")\n",
    "            return None  # Or handle error differently\n",
    "\n",
    "    @staticmethod\n",
    "    def to_mfcc(wav):\n",
    "        \"\"\"\n",
    "            Converts a wav file to Mel Frequency Ceptral Coefficients (MFCCs).\n",
    "            Args:\n",
    "                wav (numpy array): The wav form data.\n",
    "                sr (int, optional): The sample rate of the audio. Defaults to None (use from data if available).\n",
    "                n_mfcc (int, optional): The number of MFCC coefficients to extract. Defaults to None (use librosa's default).\n",
    "            Returns:\n",
    "                numpy.ndarray: A 2D numpy array containing the MFCC features.\n",
    "            Raises:\n",
    "                Exception: If an error occurs during processing.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return librosa.feature.mfcc(y=wav, sr=RATE, n_mfcc=N_MFCC)\n",
    "        except Exception as e:\n",
    "            print(f\"Error converting wav to MFCC: {e}\")\n",
    "            return None  # Or handle error differently\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_mfcc(mfcc):\n",
    "        '''\n",
    "            Normalize mfcc\n",
    "            :param mfcc:\n",
    "            :return:\n",
    "        '''\n",
    "        mms = MinMaxScaler()\n",
    "        return(mms.fit_transform(np.abs(mfcc)))\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_silence(wav, thresh=0.04, chunk=5000):\n",
    "        '''\n",
    "            Searches wav form for segments of silence. If wav form values are lower than 'thresh' for 'chunk' samples, the values will be removed\n",
    "            :param wav (np array): Wav array to be filtered\n",
    "            :return (np array): Wav array with silence removed\n",
    "        '''\n",
    "\n",
    "        tf_list = []\n",
    "        for x in range(len(wav) / chunk):\n",
    "            if (np.any(wav[chunk * x:chunk * (x + 1)] >= thresh) or np.any(wav[chunk * x:chunk * (x + 1)] <= -thresh)):\n",
    "                tf_list.extend([True] * chunk)\n",
    "            else:\n",
    "                tf_list.extend([False] * chunk)\n",
    "\n",
    "        tf_list.extend((len(wav) - len(tf_list)) * [False])\n",
    "        return(wav[tf_list])\n",
    "\n",
    "    @staticmethod\n",
    "    def make_segments(mfccs,labels):\n",
    "        '''\n",
    "            Makes segments of mfccs and attaches them to the labels\n",
    "            :param mfccs: list of mfccs\n",
    "            :param labels: list of labels\n",
    "            :return (tuple): Segments with labels\n",
    "        '''\n",
    "        segments = []\n",
    "        seg_labels = []\n",
    "        for mfcc,label in zip(mfccs,labels):\n",
    "            for start in range(0, int(mfcc.shape[1] / COL_SIZE)):\n",
    "                segments.append(mfcc[:, start * COL_SIZE:(start + 1) * COL_SIZE])\n",
    "                seg_labels.append(label)\n",
    "        return (segments, seg_labels)\n",
    "\n",
    "    @staticmethod\n",
    "    def segment_one(mfcc):\n",
    "        '''\n",
    "            Creates segments from on mfcc image. If last segments is not long enough to be length of columns divided by COL_SIZE\n",
    "            :param mfcc (numpy array): MFCC array\n",
    "            :return (numpy array): Segmented MFCC array\n",
    "        '''\n",
    "        segments = []\n",
    "        for start in range(0, int(mfcc.shape[1] / COL_SIZE)):\n",
    "            segments.append(mfcc[:, start * COL_SIZE:(start + 1) * COL_SIZE])\n",
    "        return(np.array(segments))\n",
    "\n",
    "    @staticmethod\n",
    "    def create_segmented_mfccs(X_train):\n",
    "        '''\n",
    "            Creates segmented MFCCs from X_train\n",
    "            :param X_train: list of MFCCs\n",
    "            :return: segmented mfccs\n",
    "        '''\n",
    "        segmented_mfccs = []\n",
    "        for mfcc in X_train:\n",
    "            segmented_mfccs.append(FeatureExtractorUtils.segment_one(mfcc))\n",
    "        return(segmented_mfccs)\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_mfcc_lst(audio_segment, sr=22050, n_mfcc=13, mel_filter_bank=20):\n",
    "        \"\"\"\n",
    "            Extracts MFCCs from a given audio segment.\n",
    "            Args:\n",
    "                :param audio_segment: A NumPy array of the audio data.\n",
    "                :param sr: Sampling rate of the audio (default: 22050 Hz).\n",
    "                :param n_mfcc: Number of MFCC coefficients to extract (default: 13).\n",
    "                :param mel_filter_bank: Number of mel filters to use (default: 20).\n",
    "            Returns:\n",
    "                A NumPy array of MFCC features for the audio segment.\n",
    "        \"\"\"\n",
    "        mel_spectrogram = librosa.feature.melspectrogram(\n",
    "            audio_segment, sr=sr, n_mels=mel_filter_bank\n",
    "        )\n",
    "        mfcc_lst = librosa.feature.mfcc(\n",
    "            S=librosa.power_to_db(mel_spectrogram), \n",
    "            n_mfccs=n_mfcc\n",
    "        )\n",
    "        return mfcc_lst.T  # Transpose to have features as columns\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_mfcc_lst(mfcc_lst: np.array, method: str = None) -> np.array:\n",
    "        \"\"\"\n",
    "            Normalizes MFCC features.\n",
    "\n",
    "            Args:\n",
    "                :param mfcc_lst: A NumPy array of MFCC features.\n",
    "                :param method: Normalization method (default: None) -> This will use the MinMaxScaler.\n",
    "                        Supported methods: 'minmax', 'standard'\n",
    "\n",
    "            Returns:\n",
    "                :return: The normalized MFCC features.\n",
    "        \"\"\"\n",
    "        if method == 'minmax':\n",
    "            return (mfcc_lst - np.min(mfcc_lst)) / (np.max(mfcc_lst) - np.min(mfcc_lst))\n",
    "        elif method == 'standard':\n",
    "            return (mfcc_lst - np.mean(mfcc_lst)) / np.std(mfcc_lst)\n",
    "        else:\n",
    "            mms = MinMaxScaler()\n",
    "            return mms.fit_transform(np.abs(mfcc_lst))\n",
    "\n",
    "    @staticmethod\n",
    "    def reduce_dimensionality(mfcc_lst: np.array, n_components: int = 10) -> np.array:\n",
    "        \"\"\"\n",
    "            Reduces the dimensionality of MFCC features using PCA.\n",
    "            The number of principal components to retain is specified by n_components.\n",
    "\n",
    "            Args:\n",
    "                :param mfcc_lst: A NumPy array of MFCC features.\n",
    "                :param n_components: Number of principal components to retain (default: 10).\n",
    "\n",
    "            Returns:\n",
    "                :return: The reduced MFCC features.\n",
    "        \"\"\"\n",
    "        pca = PCA(n_components=n_components)\n",
    "        reduced_mfcc_lst = pca.fit_transform(mfcc_lst)\n",
    "        return reduced_mfcc_lst\n",
    "\n",
    "    @staticmethod\n",
    "    def combine_segments(mfcc_segments: list, method='mean'):\n",
    "        \"\"\"\n",
    "            Combines MFCC features from multiple segments.\n",
    "            Args:\n",
    "                :param mfcc_segments: A list of NumPy arrays of MFCC features, one for each segment.\n",
    "                :param method: Method for combining features (default: 'mean').\n",
    "                        Supported methods: 'mean', 'concatenate'\n",
    "\n",
    "            Returns:\n",
    "                :return: A NumPy array of combined MFCC features.\n",
    "        \"\"\"\n",
    "        if method == 'mean':\n",
    "            return np.mean(mfcc_segments, axis=0)\n",
    "        elif method == 'concatenate':\n",
    "            return np.concatenate(mfcc_segments, axis=1)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid combination method: {method}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def filter_df(dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "            Function to filter audio files based on df columns\n",
    "            df column options: [age,age_of_english_onset,age_sex,birth_place,english_learning_method,\n",
    "            english_residence,length_of_english_residence,native_language,other_languages,sex]\n",
    "            :return (DataFrame): Filtered DataFrame\n",
    "        \"\"\"\n",
    "\n",
    "        # if the dataframe['native_language'] has arabic, mandarin, or english then\n",
    "        arabic = dataframe[dataframe.native_language == 'arabic']\n",
    "        mandarin = dataframe[dataframe.native_language == 'mandarin']\n",
    "        english = dataframe[dataframe.native_language == 'english']    \n",
    "        mandarin = mandarin[mandarin.length_of_english_residence < 10]\n",
    "        arabic = arabic[arabic.length_of_english_residence < 10]\n",
    "        # use concat to add the dataframes together\n",
    "        return pd.concat(\n",
    "            [\n",
    "                dataframe,\n",
    "                mandarin,\n",
    "                arabic,\n",
    "                english,\n",
    "            ],\n",
    "            ignore_index=True\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def split_people(dataframe: pd.DataFrame, test_size: float = 0.2, second_column: str = 'english_residence'):\n",
    "        \"\"\"\n",
    "            Create train test split of DataFrame\n",
    "            Args:\n",
    "                :param dataframe: DataFrame to be split\n",
    "                :param test_size: Percentage of total files to be split into test\n",
    "            Return:\n",
    "                :return X_train, X_test, y_train, y_test (tuple): Xs are list of\n",
    "                df['language_num'] and Ys are df['english_residence']\n",
    "        \"\"\"\n",
    "        x_train, x_test, y_train, y_test = train_test_split(\n",
    "            dataframe['language_num'],\n",
    "            dataframe[second_column],\n",
    "            test_size=test_size,\n",
    "            train_size= 1 - test_size,\n",
    "            random_state=1234\n",
    "        )\n",
    "        return x_train, x_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioFeatureExtractor:\n",
    "    def __init__(self, path:str) -> None:\n",
    "        \"\"\"\n",
    "            Initializes the feature extractor with the directory containing audio files of native speakers.\n",
    "            Args:\n",
    "                path (str): Directory path containing audio files of the native English speakers.\n",
    "        \"\"\"\n",
    "        self.native_speakers_dir = path\n",
    "        self.native_speakers_data = self.load_native_speakers_files()\n",
    "\n",
    "    def load_native_speakers_files(self):\n",
    "        \"\"\"\n",
    "            Loads the file paths of all audio files in the native speakers directory.\n",
    "            Returns:\n",
    "                list: A list containing the file paths of the native English speakers' audio files.\n",
    "        \"\"\"\n",
    "        # This will list every file in the directory\n",
    "        file_paths = [file for file in os.listdir(self.native_speakers_dir) if file.endswith('.wav')]\n",
    "        return file_paths\n",
    "\n",
    "    \n",
    "    def combine_native_speakers(self, file_path, num_samples=1):\n",
    "        \"\"\"\n",
    "            Combines a 1-second voice sample from the specified file with another 1-second sample\n",
    "            from a randomly selected native speaker's voice to create a 2-second sample.\n",
    "            Args:\n",
    "                file_path (str): Path to the primary audio file.\n",
    "                num_samples (int): Number of 1-second samples to combine with, should be 1 for a 2-second sample.\n",
    "            Returns:\n",
    "                np.ndarray: The combined 2-second audio waveform.\n",
    "        \"\"\"\n",
    "        # print(os.path.join(self.native_speakers_dir, file_path))\n",
    "        try:\n",
    "            # Load the primary audio file and take a 1-second sample\n",
    "            primary_y, sr = librosa.load(os.path.join(self.native_speakers_dir, file_path), sr=None)\n",
    "            # primary_one_second = librosa.util.fix_length(primary_y, sr)[:sr]\n",
    "            primary_one_second = librosa.util.fix_length(primary_y, size=num_samples)\n",
    "\n",
    "            # Randomly select another native speaker file and take a 1-second sample\n",
    "            random_file_path = random.choice(self.native_speakers_data)\n",
    "            random_y, _ = librosa.load(os.path.join(self.native_speakers_dir, random_file_path), sr=None)\n",
    "            # random_one_second = librosa.util.fix_length(random_y, sr)[:sr]\n",
    "            random_one_second = librosa.util.fix_length(random_y, size=num_samples)\n",
    "\n",
    "            # Concatenate the two 1-second samples to create a 2-second sample\n",
    "            combined_y = np.concatenate((primary_one_second, random_one_second))\n",
    "            return combined_y, sr\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error combining native speaker samples for {file_path}: {e}\")\n",
    "            return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # audio feature extraction instance\n",
    "# audio_feature_extractor = AudioFeatureExtractor(path=NATIVE_DIR)\n",
    "# # load native speaker files\n",
    "# native_speakers_data = audio_feature_extractor.load_native_speakers_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # NB: Only run this cell once to combine all native speakers\n",
    "\n",
    "# # load all native speakers\n",
    "# # native_speakers = []\n",
    "# # native_sr = []\n",
    "# os.makedirs('./data/native_combined', exist_ok=True)\n",
    "# for file in native_speakers_data:\n",
    "#     combined_y, sr = audio_feature_extractor.combine_native_speakers(file_path = file, num_samples = 1)\n",
    "#     # native_speakers.append(combined_y)\n",
    "#     # native_sr.append(sr)\n",
    "#     sf.write(f'./data/native_combined/{file}', combined_y, sr, subtype='PCM_24')\n",
    "#     # librosa.output.write_wav(f'data/audio/combined/{file}', combined_y, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the native_bio_metadata.csv\n",
    "native_bio_metadata = pd.read_csv(NATIVE_FILE_PATH)\n",
    "native_bio_metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureExtractorUtils = FeatureExtractorUtils()\n",
    "xFeatureExtractor = XFeatureExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the native_combined_df into train and test\n",
    "X_train, X_test, y_train, y_test = featureExtractorUtils.split_people(native_bio_metadata, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test.apply(lambda x: x.split('\\n')[0])\n",
    "y_train = y_train.apply(lambda x: x.split('\\n')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get statistics\n",
    "train_count = Counter(y_train)\n",
    "test_count = Counter(y_test)\n",
    "\n",
    "\n",
    "print(f\"Train Count: {train_count}\")\n",
    "print(f\"Test Count: {test_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train_encoded = LabelEncoder().fit_transform(y_train)\n",
    "# y_test_encoded = LabelEncoder().fit_transform(y_test)\n",
    "# y_train_cat = to_categorical(y_train_encoded)\n",
    "# y_test_cat = to_categorical(y_test_encoded)\n",
    "\n",
    "y_train_cat, _ = featureExtractorUtils.to_categorical(y_train)\n",
    "y_test_cat, lang_dict = featureExtractorUtils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Prosodic, Accoustic and PLP features from audio files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ThreadPoolExecutor() as pool:\n",
    "    X_prosodic = pool.map(xFeatureExtractor.extract_prosodic_features, X_train)\n",
    "    X_acoustic = pool.map(xFeatureExtractor.extract_acoustic_features, X_train)\n",
    "    X_plp = pool.map(xFeatureExtractor.extract_plp, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ThreadPoolExecutor() as pool:\n",
    "    X_test_prosodic = pool.map(xFeatureExtractor.extract_prosodic_features, X_test)\n",
    "    X_test_acoustic = pool.map(xFeatureExtractor.extract_acoustic_features, X_test)\n",
    "    X_test_plp = pool.map(xFeatureExtractor.extract_plp, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on SHARED Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_acoustic_list = list(X_acoustic)\n",
    "X_prosodic_list = list(X_prosodic)\n",
    "X_plp_list = list(X_plp)\n",
    "\n",
    "print(\"Type of X_acoustic:\", type(X_acoustic))\n",
    "print(\"Length of X_acoustic:\", len(X_acoustic_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_acoustic_test_list = list(X_test_acoustic)\n",
    "X_prosodic_test_list = list(X_test_prosodic)\n",
    "X_plp_test_list = list(X_test_plp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_model(X_train_np:np.array):\n",
    "    # L2 regularization factor\n",
    "    l2_reg = regularizers.l2(1e-4)\n",
    "    # Separate Input Channels approach\n",
    "    input_shape = X_train_np.shape[1:]  # Use the shape of the MFCC arrays\n",
    "    mfcc_in = keras.layers.Input(shape=input_shape)\n",
    "    mfcc_x = keras.layers.Conv1D(filters=16, kernel_size=3, activation='relu')(mfcc_in)\n",
    "    merged = keras.layers.concatenate([mfcc_x])\n",
    "    # merged = mfcc_in\n",
    "\n",
    "    # Rest of the model definition (common for both approaches)\n",
    "    x = keras.layers.MaxPooling1D(pool_size=2)(merged)\n",
    "    x = keras.layers.Conv1D(filters=8, kernel_size=3, activation='relu')(x)\n",
    "    x = keras.layers.MaxPooling1D(pool_size=2)(x)\n",
    "    x = keras.layers.Dropout(0.5)(x)\n",
    "    x = keras.layers.Flatten()(x)\n",
    "    x = keras.layers.Dense(\n",
    "        units=16, \n",
    "        activation='relu',\n",
    "        kernel_regularizer=l2_reg\n",
    "    )(x)\n",
    "    outputs = keras.layers.Dense(\n",
    "        3, activation='softmax',\n",
    "        kernel_regularizer=l2_reg\n",
    "    )(x)  # 3 for usa, uk, canada classes\n",
    "\n",
    "    model = keras.Model(\n",
    "        inputs=[mfcc_in], outputs=outputs\n",
    "    )\n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy', \n",
    "        optimizer='adam', \n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_multi_feature_model(\n",
    "    X_mfcc_np:np.array, \n",
    "    X_chroma_stft_np:np.array = None,\n",
    "    X_spectral_centroid_np:np.array = None,\n",
    "    use_separate_channels:bool = False\n",
    "):\n",
    "    \n",
    "    if use_separate_channels and ((X_chroma_stft_np is not None) and (X_spectral_centroid_np is not None)):\n",
    "        # Separate Input Channels approach\n",
    "        input_shape_mfcc = X_mfcc_np.shape[1:]\n",
    "        input_shape_chroma_stft = X_chroma_stft_np.shape[1:]\n",
    "        input_shape_spectral_centroid = X_spectral_centroid_np.shape[1:]\n",
    "\n",
    "        mfcc_in = keras.layers.Input(shape=input_shape_mfcc)\n",
    "        chroma_stft_in = keras.layers.Input(shape=input_shape_chroma_stft)\n",
    "        spectral_centroid_in = keras.layers.Input(shape=input_shape_spectral_centroid)\n",
    "\n",
    "        mfcc_x = keras.layers.Conv1D(filters=32, kernel_size=3, activation='relu', padding='same')(mfcc_in)\n",
    "        chroma_stft_x = keras.layers.Conv1D(\n",
    "            filters=16, kernel_size=3, activation='relu', padding='same'\n",
    "        )(chroma_stft_in)\n",
    "        spectral_centroid_x = keras.layers.Conv1D(\n",
    "            filters=8, kernel_size=3, activation='relu', padding='same'\n",
    "        )(spectral_centroid_in)\n",
    "        \n",
    "        # mfcc_x = keras.layers.GlobalAveragePooling1D()(mfcc_x)\n",
    "        # chroma_stft_x = keras.layers.GlobalAveragePooling1D()(chroma_stft_x)\n",
    "        # spectral_centroid_x = keras.layers.GlobalAveragePooling1D()(spectral_centroid_x)\n",
    "        \n",
    "        merged = keras.layers.concatenate([mfcc_x, chroma_stft_x, spectral_centroid_x])\n",
    "        \n",
    "    elif use_separate_channels and (X_chroma_stft_np is not None):\n",
    "        # Separate Input Channels approach\n",
    "        input_shape_mfcc = X_mfcc_np.shape[1:]\n",
    "        input_shape_chroma_stft = X_chroma_stft_np.shape[1:]\n",
    "\n",
    "        mfcc_in = keras.layers.Input(shape=input_shape_mfcc)\n",
    "        chroma_stft_in = keras.layers.Input(shape=input_shape_chroma_stft)\n",
    "\n",
    "        mfcc_x = keras.layers.Conv1D(\n",
    "            filters=16, kernel_size=3, activation='relu', padding='same'\n",
    "        )(mfcc_in)\n",
    "        chroma_stft_x = keras.layers.Conv1D(\n",
    "            filters=8, kernel_size=3, activation='relu', padding='same'\n",
    "        )(chroma_stft_in)\n",
    "        \n",
    "        # Apply global average pooling to reduce the dimensionality\n",
    "        # mfcc_x = keras.layers.GlobalAveragePooling1D()(mfcc_x)\n",
    "        # chroma_stft_x = keras.layers.GlobalAveragePooling1D()(chroma_stft_x)\n",
    "        \n",
    "        merged = keras.layers.concatenate([mfcc_x, chroma_stft_x])\n",
    "        \n",
    "    elif use_separate_channels and (X_spectral_centroid_np is not None):\n",
    "        # Separate Input Channels approach\n",
    "        input_shape_mfcc = X_mfcc_np.shape[1:]\n",
    "        input_shape_spectral_centroid = X_spectral_centroid_np.shape[1:]\n",
    "\n",
    "        mfcc_in = keras.layers.Input(shape=input_shape_mfcc)\n",
    "        spectral_centroid_in = keras.layers.Input(shape=input_shape_spectral_centroid)\n",
    "\n",
    "        mfcc_x = keras.layers.Conv1D(filters=16, kernel_size=3, activation='relu', padding='same')(mfcc_in)\n",
    "        spectral_centroid_x = keras.layers.Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(spectral_centroid_in)\n",
    "        \n",
    "        # mfcc_x = keras.layers.GlobalAveragePooling1D()(mfcc_x)\n",
    "        # chroma_stft_x = keras.layers.GlobalAveragePooling1D()(chroma_stft_x)\n",
    "\n",
    "        merged = keras.layers.concatenate([mfcc_x, spectral_centroid_x])\n",
    "    else:\n",
    "        # Concatenation approach (assuming all features have same time-frequency dimensions)\n",
    "        input_shape = X_mfcc_np.shape[1:]\n",
    "        mfcc_in = keras.layers.Input(shape=input_shape)\n",
    "        merged = mfcc_in\n",
    "    l2_reg = regularizers.l2(1e-4)\n",
    "    # Rest of the model definition (common for both approaches)\n",
    "    x = keras.layers.MaxPooling1D(pool_size=2)(merged)\n",
    "    x = keras.layers.Conv1D(filters=32, kernel_size=3, activation='relu')(x)\n",
    "    x = keras.layers.MaxPooling1D(pool_size=2)(x)\n",
    "    x = keras.layers.Dropout(0.5)(x)\n",
    "    x = keras.layers.Flatten()(x)\n",
    "    x = keras.layers.Dense(\n",
    "        units=64, activation='relu',\n",
    "        kernel_regularizer=l2_reg\n",
    "    )(x)\n",
    "    outputs = keras.layers.Dense(\n",
    "        3, activation='softmax',\n",
    "        kernel_regularizer=l2_reg\n",
    "    )(x)  # 3 for usa, uk, canada classes\n",
    "\n",
    "    if use_separate_channels and ((X_chroma_stft_np is not None) and (X_spectral_centroid_np is not None)):\n",
    "        model = keras.Model(inputs=[mfcc_in, chroma_stft_in, spectral_centroid_in], outputs=outputs)\n",
    "    elif use_separate_channels and (X_chroma_stft_np is not None):\n",
    "        model = keras.Model(inputs=[mfcc_in, chroma_stft_in], outputs=outputs)\n",
    "    elif use_separate_channels and(X_spectral_centroid_np is not None):\n",
    "        model = keras.Model(inputs=[mfcc_in, spectral_centroid_in], outputs=outputs)\n",
    "    else:\n",
    "        model = keras.Model(inputs=mfcc_in, outputs=outputs)\n",
    "\n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer='adam',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def create_hybrid_model(\n",
    "    X_mfcc_np:np.array, \n",
    "    X_chroma_stft_np:np.array = None,\n",
    "    X_spectral_centroid_np:np.array = None,\n",
    "    use_separate_channels:bool = False\n",
    ") -> any:\n",
    "    if use_separate_channels and ((X_chroma_stft_np is not None) and (X_spectral_centroid_np is not None)):\n",
    "        # Separate Input Channels approach\n",
    "        input_shape_mfcc = X_mfcc_np.shape[1:]\n",
    "        input_shape_chroma_stft = X_chroma_stft_np.shape[1:]\n",
    "        input_shape_spectral_centroid = X_spectral_centroid_np.shape[1:]\n",
    "\n",
    "        mfcc_in = keras.layers.Input(shape=input_shape_mfcc)\n",
    "        chroma_stft_in = keras.layers.Input(shape=input_shape_chroma_stft)\n",
    "        spectral_centroid_in = keras.layers.Input(shape=input_shape_spectral_centroid)\n",
    "\n",
    "        mfcc_x = keras.layers.Conv1D(\n",
    "            filters=32, kernel_size=3, activation='relu', padding='same'\n",
    "        )(mfcc_in)\n",
    "        chroma_stft_x = keras.layers.Conv1D(\n",
    "            filters=16, kernel_size=3, activation='relu', padding='same'\n",
    "        )(chroma_stft_in)\n",
    "        spectral_centroid_x = keras.layers.Conv1D(\n",
    "            filters=8, kernel_size=3, activation='relu', padding='same'\n",
    "        )(spectral_centroid_in)\n",
    "        \n",
    "        # mfcc_x = keras.layers.GlobalAveragePooling1D()(mfcc_x)\n",
    "        # chroma_stft_x = keras.layers.GlobalAveragePooling1D()(chroma_stft_x)\n",
    "        # spectral_centroid_x = keras.layers.GlobalAveragePooling1D()(spectral_centroid_x)\n",
    "        \n",
    "        merged = keras.layers.concatenate([mfcc_x, chroma_stft_x, spectral_centroid_x])\n",
    "        \n",
    "    elif use_separate_channels and (X_chroma_stft_np is not None):\n",
    "        # Separate Input Channels approach\n",
    "        input_shape_mfcc = X_mfcc_np.shape[1:]\n",
    "        input_shape_chroma_stft = X_chroma_stft_np.shape[1:]\n",
    "\n",
    "        mfcc_in = keras.layers.Input(shape=input_shape_mfcc)\n",
    "        chroma_stft_in = keras.layers.Input(shape=input_shape_chroma_stft)\n",
    "\n",
    "        mfcc_x = keras.layers.Conv1D(filters=16, kernel_size=3, activation='relu', padding='same')(mfcc_in)\n",
    "        chroma_stft_x = keras.layers.Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(chroma_stft_in)\n",
    "        \n",
    "        # Apply global average pooling to reduce the dimensionality\n",
    "        # mfcc_x = keras.layers.GlobalAveragePooling1D()(mfcc_x)\n",
    "        # chroma_stft_x = keras.layers.GlobalAveragePooling1D()(chroma_stft_x)\n",
    "        \n",
    "        merged = keras.layers.concatenate([mfcc_x, chroma_stft_x])\n",
    "        \n",
    "    elif use_separate_channels and (X_spectral_centroid_np is not None):\n",
    "        # Separate Input Channels approach\n",
    "        input_shape_mfcc = X_mfcc_np.shape[1:]\n",
    "        input_shape_spectral_centroid = X_spectral_centroid_np.shape[1:]\n",
    "\n",
    "        mfcc_in = keras.layers.Input(shape=input_shape_mfcc)\n",
    "        spectral_centroid_in = keras.layers.Input(shape=input_shape_spectral_centroid)\n",
    "\n",
    "        mfcc_x = keras.layers.Conv1D(filters=16, kernel_size=3, activation='relu', padding='same')(mfcc_in)\n",
    "        spectral_centroid_x = keras.layers.Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(spectral_centroid_in)\n",
    "        \n",
    "        # mfcc_x = keras.layers.GlobalAveragePooling1D()(mfcc_x)\n",
    "        # chroma_stft_x = keras.layers.GlobalAveragePooling1D()(chroma_stft_x)\n",
    "\n",
    "        merged = keras.layers.concatenate([mfcc_x, spectral_centroid_x])\n",
    "    else:\n",
    "        # Concatenation approach (assuming all features have same time-frequency dimensions)\n",
    "        input_shape = X_mfcc_np.shape[1:]\n",
    "        mfcc_in = keras.layers.Input(shape=input_shape)\n",
    "        merged = mfcc_in\n",
    "        \n",
    "        \n",
    "    l2_reg = regularizers.l2(1e-4)\n",
    "    # Rest of the model definition (common for both approaches)\n",
    "    x = keras.layers.MaxPooling1D(pool_size=2)(merged)\n",
    "    x = keras.layers.Conv1D(filters=32, kernel_size=3, activation='relu',\n",
    "                            kernel_regularizer=l2_reg)(x)\n",
    "    x = keras.layers.MaxPooling1D(pool_size=2)(x)\n",
    "    x = keras.layers.Dropout(0.5)(x)\n",
    "    x = keras.layers.LSTM(128, return_sequences=True)(x)\n",
    "    x = keras.layers.LSTM(128)(x)\n",
    "    x = keras.layers.Dense(units=64, activation='relu')(x)\n",
    "    outputs = keras.layers.Dense(3, activation='softmax')(x)  # 3 for usa, uk, canada classes\n",
    "\n",
    "    if use_separate_channels and ((X_chroma_stft_np is not None) and (X_spectral_centroid_np is not None)):\n",
    "        model = keras.Model(inputs=[mfcc_in, chroma_stft_in, spectral_centroid_in], outputs=outputs)\n",
    "    elif use_separate_channels and (X_chroma_stft_np is not None):\n",
    "        model = keras.Model(inputs=[mfcc_in, chroma_stft_in], outputs=outputs)\n",
    "    elif use_separate_channels and(X_spectral_centroid_np is not None):\n",
    "        model = keras.Model(inputs=[mfcc_in, spectral_centroid_in], outputs=outputs)\n",
    "    else:\n",
    "        model = keras.Model(inputs=mfcc_in, outputs=outputs)\n",
    "\n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer='adam',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=2, \n",
    "    # restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for multi-feature model use below cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 2000\n",
    "chroma_stft_list = []\n",
    "spectral_centroid_list = []\n",
    "mfcc_list = []\n",
    "\n",
    "for sample in X_acoustic_list:\n",
    "    mfcc_array = np.array(sample['mfcc'])\n",
    "    chroma_stft_array = np.array(sample['chroma_stft'])\n",
    "    spectral_centroid_array = np.array(sample['spectral_centroid'])\n",
    "    \n",
    "    if mfcc_array.shape[1] < max_length:\n",
    "        padded_mfcc_array = np.pad(mfcc_array, ((0, 0), (0, max_length - mfcc_array.shape[1])), mode='constant')\n",
    "    else:\n",
    "        padded_mfcc_array = mfcc_array[:, :max_length]\n",
    "\n",
    "    # Pad or truncate arrays to max_length if needed\n",
    "    if chroma_stft_array.shape[1] < max_length:\n",
    "        padded_chroma_stft_array = np.pad(chroma_stft_array, ((0, 0), (0, max_length - chroma_stft_array.shape[1])), mode='constant')\n",
    "    else:\n",
    "        padded_chroma_stft_array = chroma_stft_array[:, :max_length]\n",
    "\n",
    "    if spectral_centroid_array.shape[1] < max_length:\n",
    "        padded_spectral_centroid_array = np.pad(spectral_centroid_array, ((0, 0), (0, max_length - spectral_centroid_array.shape[1])), mode='constant')\n",
    "    else:\n",
    "        padded_spectral_centroid_array = spectral_centroid_array[:, :max_length]\n",
    "\n",
    "    mfcc_list.append(padded_mfcc_array)\n",
    "    chroma_stft_list.append(padded_chroma_stft_array)\n",
    "    spectral_centroid_list.append(padded_spectral_centroid_array)\n",
    "\n",
    "# Convert the lists of processed features into numpy arrays\n",
    "X_mfcc_stft_np = np.array(mfcc_list)\n",
    "X_chroma_stft_np = np.array(chroma_stft_list)\n",
    "X_spectral_centroid_np = np.array(spectral_centroid_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Test values\n",
    "\n",
    "max_length = 2000\n",
    "mfcc_test_list = []\n",
    "chroma_test_stft_list = []\n",
    "spectral_test_centroid_list = []\n",
    "\n",
    "for sample in X_acoustic_test_list:\n",
    "    mfcc_test_array = np.array(sample['mfcc'])\n",
    "    chroma_test_stft_array = np.array(sample['chroma_stft'])\n",
    "    spectral_test_centroid_array = np.array(sample['spectral_centroid'])\n",
    "    \n",
    "    if mfcc_test_array.shape[1] < max_length:\n",
    "        padded_mfcc_test_array = np.pad(mfcc_test_array, ((0, 0), (0, max_length - mfcc_test_array.shape[1])), mode='constant')\n",
    "    else:\n",
    "        padded_mfcc_test_array = mfcc_test_array[:, :max_length]\n",
    "\n",
    "    # Pad or truncate arrays to max_length if needed\n",
    "    if chroma_test_stft_array.shape[1] < max_length:\n",
    "        padded_chroma_test_stft_array = np.pad(chroma_test_stft_array, ((0, 0), (0, max_length - chroma_test_stft_array.shape[1])), mode='constant')\n",
    "    else:\n",
    "        padded_chroma_test_stft_array = chroma_test_stft_array[:, :max_length]\n",
    "\n",
    "    if spectral_test_centroid_array.shape[1] < max_length:\n",
    "        padded_spectral_test_centroid_array = np.pad(spectral_test_centroid_array, ((0, 0), (0, max_length - spectral_test_centroid_array.shape[1])), mode='constant')\n",
    "    else:\n",
    "        padded_spectral_test_centroid_array = spectral_test_centroid_array[:, :max_length]\n",
    "\n",
    "    mfcc_test_list.append(padded_mfcc_test_array)\n",
    "    chroma_test_stft_list.append(padded_chroma_test_stft_array)\n",
    "    spectral_test_centroid_list.append(padded_spectral_test_centroid_array)\n",
    "\n",
    "# Convert the lists of processed features into numpy arrays\n",
    "X_mfcc_stft_test_np = np.array(mfcc_test_list)\n",
    "X_chroma_stft_test_np = np.array(chroma_test_stft_list)\n",
    "X_spectral_centroid_test_np = np.array(spectral_test_centroid_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad or truncate the arrays to ensure they have the same shape along the concatenation axis\n",
    "max_length = max(X_mfcc_stft_np.shape[1], X_chroma_stft_np.shape[1])\n",
    "X_mfcc_np_padded = np.pad(\n",
    "    X_mfcc_stft_np, \n",
    "    ((0, 0), (0, max_length - X_mfcc_stft_np.shape[1]), (0, 0)), \n",
    "    mode='constant'\n",
    ")\n",
    "X_chroma_stft_np_padded = np.pad(\n",
    "    X_chroma_stft_np, \n",
    "    ((0, 0), (0, max_length - X_chroma_stft_np.shape[1]), (0, 0)), \n",
    "    mode='constant'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad or truncate the arrays to ensure they have the same shape along the concatenation axis\n",
    "max_length_test = max(X_mfcc_stft_test_np.shape[1], X_chroma_stft_test_np.shape[1])\n",
    "X_mfcc_test_np_padded = np.pad(\n",
    "    X_mfcc_stft_test_np, \n",
    "    ((0, 0), (0, max_length_test - X_mfcc_stft_test_np.shape[1]), (0, 0)), \n",
    "    mode='constant'\n",
    ")\n",
    "X_chroma_test_stft_np_padded = np.pad(\n",
    "    X_chroma_stft_test_np,\n",
    "    ((0, 0), (0, max_length_test - X_chroma_stft_test_np.shape[1]), (0, 0)), \n",
    "    mode='constant'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the approach (concatenation or separate channels)\n",
    "use_separate_channels = True\n",
    "\n",
    "# Create CNN architecture\n",
    "multi_model = create_multi_feature_model(\n",
    "    X_mfcc_np=X_mfcc_np_padded,\n",
    "    X_chroma_stft_np=X_chroma_stft_np_padded,\n",
    "    # X_spectral_centroid_np=X_spectral_centroid_np,\n",
    "    use_separate_channels=use_separate_channels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model summary\n",
    "multi_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"files\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the model\n",
    "keras.utils.plot_model(multi_model, to_file='files/multi_feature_model.png', show_shapes=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = multi_model.fit(\n",
    "    [X_mfcc_np_padded, X_chroma_stft_np_padded], \n",
    "    y_train_cat,\n",
    "    epochs=EPOCHS_MULTI,\n",
    "    validation_data=([X_mfcc_test_np_padded, X_chroma_test_stft_np_padded], y_test_cat),\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "multi_model.save('./files/multi_feature_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "loss, accuracy = multi_model.evaluate([\n",
    "        X_mfcc_test_np_padded, \n",
    "        X_chroma_test_stft_np_padded\n",
    "    ],\n",
    "    y_test_cat\n",
    ")\n",
    "print(\"Test Loss:\", loss, \"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on new data\n",
    "multi_predictions = multi_model.predict([\n",
    "    X_mfcc_test_np_padded, \n",
    "    X_chroma_test_stft_np_padded\n",
    "])\n",
    "\n",
    "# The predictions will be probabilities for each class\n",
    "# You can convert these probabilities to class labels\n",
    "multi_predicted_labels = np.argmax(multi_predictions, axis=1)\n",
    "\n",
    "# Example of using the predicted labels\n",
    "print(multi_predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_index_to_label = {0: 'usa', 1: 'canada', 2: 'uk'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert numeric predictions to label names\n",
    "predicted_labels_names = [class_index_to_label[idx] for idx in np.argmax(multi_predictions, axis=1)]\n",
    "\n",
    "predicted_labels_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert predicted class indices to one-hot encoded format\n",
    "label_binarizer = LabelBinarizer()\n",
    "predicted_labels_one_hot = label_binarizer.fit_transform(multi_predicted_labels)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(y_test_cat.argmax(axis=1), predicted_labels_one_hot.argmax(axis=1))\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    cm, \n",
    "    annot=True, \n",
    "    cmap='Blues', \n",
    "    fmt='g', \n",
    "    xticklabels=class_index_to_label.values(), \n",
    "    yticklabels=class_index_to_label.values()\n",
    ")\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('./files/multi_feature_confusion_matrix.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for single feature model - use below cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_acoustic is a list of dictionaries with 'mfcc' as one of the features\n",
    "max_length = 2000  # Define the maximum length you want to pad/truncate to\n",
    "X_train_acoustic_processed = []\n",
    "for sample in X_acoustic_list:\n",
    "    mfcc_array = np.array(sample['mfcc'])\n",
    "    current_length = mfcc_array.shape[1]\n",
    "    # Pad or truncate the array to max_length\n",
    "    if current_length < max_length:\n",
    "        padded_mfcc_array = np.pad(mfcc_array, ((0, 0), (0, max_length - current_length)), mode='constant')\n",
    "    else:\n",
    "        padded_mfcc_array = mfcc_array[:, :max_length]\n",
    "    X_train_acoustic_processed.append(padded_mfcc_array)\n",
    "\n",
    "# Convert the list of processed features into a numpy array\n",
    "X_train_np = np.array(X_train_acoustic_processed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X_train_acoustic is a list of dictionaries with 'mfcc' as one of the features\n",
    "max_length = 2000  # Define the maximum length you want to pad/truncate to\n",
    "X_test_acoustic_processed = []\n",
    "for sample in X_acoustic_test_list:\n",
    "    mfcc_array = np.array(sample['mfcc'])\n",
    "    current_length = mfcc_array.shape[1]\n",
    "    # Pad or truncate the array to max_length\n",
    "    if current_length < max_length:\n",
    "        padded_mfcc_array = np.pad(mfcc_array, ((0, 0), (0, max_length - current_length)), mode='constant')\n",
    "    else:\n",
    "        padded_mfcc_array = mfcc_array[:, :max_length]\n",
    "    X_test_acoustic_processed.append(padded_mfcc_array)\n",
    "\n",
    "# Convert the list of processed features into a numpy array\n",
    "X_test_np = np.array(X_test_acoustic_processed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_model = create_feature_model(X_train_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model summary\n",
    "single_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"files\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the model\n",
    "keras.utils.plot_model(single_model, to_file='files/feature_model.png', show_shapes=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "single_history = single_model.fit(\n",
    "    X_train_np, \n",
    "    y_train_cat, \n",
    "    epochs=EPOCHS_SINGLE, \n",
    "    validation_data=(X_test_np, y_test_cat),\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(single_history.history['accuracy'])\n",
    "plt.plot(single_history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation loss values\n",
    "plt.plot(single_history.history['loss'])\n",
    "plt.plot(single_history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "single_model.save('./files/feature_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "loss, accuracy = single_model.evaluate(X_test_np, y_test_cat)\n",
    "print(\"Test Loss:\", loss, \"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make predictions on new data\n",
    "predictions = single_model.predict(X_test_np)\n",
    "\n",
    "# The predictions will be probabilities for each class\n",
    "# You can convert these probabilities to class labels\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Example of using the predicted labels\n",
    "print(predicted_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert numeric predictions to label names\n",
    "predicted_labels_names = [class_index_to_label[idx] for idx in np.argmax(multi_predictions, axis=1)]\n",
    "\n",
    "predicted_labels_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert predicted class indices to one-hot encoded format\n",
    "label_binarizer = LabelBinarizer()\n",
    "labels_one_hot = label_binarizer.fit_transform(predicted_labels)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(y_test_cat.argmax(axis=1), labels_one_hot.argmax(axis=1))\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    cm, \n",
    "    annot=True, \n",
    "    cmap='Blues', \n",
    "    fmt='g', \n",
    "    xticklabels=class_index_to_label.values(), \n",
    "    yticklabels=class_index_to_label.values()\n",
    ")\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('./files/feature_confusion_matrix.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Hybrid CNN-LSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose the approach (concatenation or separate channels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_separate_channels = True\n",
    "\n",
    "# Create CNN-LSTM architecture\n",
    "hybrid_model = create_hybrid_model(\n",
    "    X_mfcc_np=X_mfcc_np_padded,\n",
    "    X_chroma_stft_np=X_chroma_stft_np_padded,\n",
    "    # X_spectral_centroid_np=X_spectral_centroid_np,\n",
    "    use_separate_channels=use_separate_channels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model summary\n",
    "hybrid_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot model\n",
    "keras.utils.plot_model(hybrid_model, to_file='files/hybrid_feature_model.png', show_shapes=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = hybrid_model.fit(\n",
    "    [X_mfcc_np_padded, X_chroma_stft_np_padded],\n",
    "    y_train_cat,\n",
    "    epochs=EPOCHS_HYBRID, # EPOCHS\n",
    "    validation_data=([X_mfcc_test_np_padded, X_chroma_test_stft_np_padded], y_test_cat),\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "hybrid_model.save('./files/hybrid_feature_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "loss, accuracy = hybrid_model.evaluate([\n",
    "        X_mfcc_test_np_padded, \n",
    "        X_chroma_test_stft_np_padded\n",
    "    ],\n",
    "    y_test_cat\n",
    ")\n",
    "print(\"Test Loss:\", loss, \"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on new data\n",
    "hybrid_predictions = hybrid_model.predict([\n",
    "    X_mfcc_test_np_padded, \n",
    "    X_chroma_test_stft_np_padded\n",
    "])\n",
    "\n",
    "# The predictions will be probabilities for each class\n",
    "# You can convert these probabilities to class labels\n",
    "hybrid_predicted_labels = np.argmax(hybrid_predictions, axis=1)\n",
    "\n",
    "# Example of using the predicted labels\n",
    "print(multi_predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_index_to_label = {0: 'usa', 1: 'canada', 2: 'uk'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert numeric predictions to label names\n",
    "predicted_labels_names = [class_index_to_label[idx] for idx in np.argmax(hybrid_predictions, axis=1)]\n",
    "\n",
    "predicted_labels_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert predicted class indices to one-hot encoded format\n",
    "label_binarizer = LabelBinarizer()\n",
    "predicted_labels_one_hot = label_binarizer.fit_transform(hybrid_predicted_labels)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(y_test_cat.argmax(axis=1), predicted_labels_one_hot.argmax(axis=1))\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    cm, \n",
    "    annot=True, \n",
    "    cmap='Blues', \n",
    "    fmt='g', \n",
    "    xticklabels=class_index_to_label.values(), \n",
    "    yticklabels=class_index_to_label.values()\n",
    ")\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('./files/hybrid_feature_confusion_matrix.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
