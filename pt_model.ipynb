{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import keras\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get and set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR: str = \"output/\"\n",
    "CSV_FILE_PATH: str = \"bio_metadata.csv\"\n",
    "NATIVE_FILE_PATH: str = \"native_bio_metadata.csv\"\n",
    "NON_NATIVE_FILE_PATH: str = \"non_native_bio_metadata.csv\"\n",
    "NATIVE_LANGUAGES: list[str] = ['uk', 'usa', 'canada']\n",
    "NON_NATIVE_LANGUAGES: list[str] = [\n",
    "    'australia',\n",
    "    'new zealand',\n",
    "    'ireland',    \n",
    "    'singapore',  \n",
    "    'south',     \n",
    "    'africa',   \n",
    "    'jamaica',    \n",
    "    'scotland',   \n",
    "    'islands',\n",
    "]\n",
    "DATASET_DIR: str = \"data/\"\n",
    "NATIVE_DIR: str = \"data/native/\"\n",
    "NATIVE_COMBINED_DIR: str = \"data/native_combined/\"\n",
    "NON_NATIVE_DIR: str = \"data/non_native/\"\n",
    "AUDIO_DATA_DIR: str = \"data/audio/\"\n",
    "AUDIO_FILE_PATH: str = \"data/audio/{}.wav\"\n",
    "SILENCE_THRESHOLD: float = .01\n",
    "RATE: int = 2400\n",
    "N_MFCC: int = 13\n",
    "COL_SIZE: int = 10\n",
    "EPOCHS: int = 50 #35 #50 #250\n",
    "LEARNING_RATE = 0.001\n",
    "WAIT: float = 1.2\n",
    "DEBUG: bool = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract acoustic features from audio files function\n",
    "def extract_mfcc_features(file_name):\n",
    "    try:\n",
    "        audio, sample_rate = librosa.load(file_name)\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=N_MFCC)\n",
    "        mfccs_processed = np.mean(mfccs.T, axis=0)\n",
    "    except Exception as e:\n",
    "        print(\"Error encountered while parsing file: \", file_name)\n",
    "        return None\n",
    "    return mfccs_processed\n",
    "\n",
    "# extract, chroma_stft, spectral_centroid, \n",
    "# spectral_bandwidth, spectral_rolloff, zero_crossing_rate function\n",
    "def extract_accoustic_features(file_name):\n",
    "    \"\"\"\n",
    "        Extracts accoustic features from audio file\n",
    "        Takes in the file name and returns the following features:\n",
    "        Args:\n",
    "            :param file_name: str: name of the file to extract features from\n",
    "        Returns:\n",
    "            :return mfcc: np.array: Mel-frequency cepstral coefficients\n",
    "            :return chroma_stft: np.array: Chroma short-time Fourier transform\n",
    "            :return spectral_centroid: np.array: Spectral centroid\n",
    "            :return spectral_bandwidth: np.array: Spectral bandwidth\n",
    "            :return spectral_rolloff: np.array: Spectral rolloff\n",
    "            :return zero_crossing_rate: np.array: Zero crossing rate\n",
    "    \"\"\"\n",
    "    try:\n",
    "        audio, sample_rate = librosa.load(f'./data/native_combined/{file_name}')\n",
    "        mfcc = np.mean(librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=COL_SIZE).T, axis=0)\n",
    "        chroma_stft = np.mean(librosa.feature.chroma_stft(y=audio, sr=sample_rate).T, axis=0)\n",
    "        spectral_centroid = np.mean(librosa.feature.spectral_centroid(y=audio, sr=sample_rate).T, axis=0)\n",
    "        spectral_bandwidth = np.mean(librosa.feature.spectral_bandwidth(y=audio, sr=sample_rate).T, axis=0)\n",
    "        spectral_rolloff = np.mean(librosa.feature.spectral_rolloff(y=audio, sr=sample_rate).T, axis=0)\n",
    "        zero_crossing_rate = np.mean(librosa.feature.zero_crossing_rate(audio).T, axis=0)\n",
    "        return mfcc, chroma_stft, spectral_centroid, spectral_bandwidth, spectral_rolloff, zero_crossing_rate\n",
    "    except Exception as e:\n",
    "        print(\"Error encountered while parsing file: \", file_name)\n",
    "        return None\n",
    "    \n",
    "\n",
    "# extract pitch intensity, duration, loudness, jitter, shimmer, hnr function (prosodic features)\n",
    "def extract_prosodic_features(file_name):\n",
    "    try:\n",
    "        audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast')\n",
    "        pitch_intensity = np.mean(librosa.pyin.piptrack(y=audio, sr=sample_rate).T, axis=0)\n",
    "        duration = np.mean(librosa.effects.time_stretch(audio, 1.0).T, axis=0)\n",
    "        loudness = np.mean(librosa.feature.rms(y=audio).T, axis=0)\n",
    "        jitter = np.mean(librosa.effects.jitter(y=audio).T, axis=0)\n",
    "        shimmer = np.mean(librosa.effects.shimmer(y=audio).T, axis=0)\n",
    "        hnr = np.mean(librosa.effects.harmonic(y=audio).T, axis=0)\n",
    "        return pitch_intensity, duration, loudness, jitter, shimmer, hnr\n",
    "    except Exception as e:\n",
    "        print(\"Error encountered while parsing file: \", file_name)\n",
    "        return None\n",
    "    \n",
    "\n",
    "# extract plp features function\n",
    "def extract_plp_features(file_name):\n",
    "    try:\n",
    "        audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast')\n",
    "        plp = np.mean(librosa.beat.plp(y=audio, sr=sample_rate, n_mfcc=COL_SIZE).T, axis=0)\n",
    "    except Exception as e:\n",
    "        print(\"Error encountered while parsing file: \", file_name)\n",
    "        return None\n",
    "    return plp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get wav from file function\n",
    "def get_wav(file_name):\n",
    "    try:\n",
    "        audio, sample_rate = librosa.load(f'./data/native_combined/{file_name}.wav')\n",
    "        return librosa.core.resample(y=audio, orig_sr=sample_rate, target_sr=RATE, scale=True)\n",
    "    except Exception as e:\n",
    "        print(\"Error encountered while parsing file: \", file_name)\n",
    "        return None\n",
    "\n",
    "# convert wave to mfcc function\n",
    "def wave_to_mfcc(audio, sample_rate):\n",
    "    try:\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=N_MFCC)\n",
    "        mfccs_processed = np.mean(mfccs.T, axis=0)\n",
    "    except Exception as e:\n",
    "        print(\"Error encountered while parsing audio\")\n",
    "        return None\n",
    "    return mfccs_processed\n",
    "\n",
    "# normalize mfcc function\n",
    "def normalize_mfcc(mfcc):\n",
    "    mms = MinMaxScaler()\n",
    "    return mms.fit_transform(np.abs(mfcc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to categorical function\n",
    "def to_categorical(y):\n",
    "    lang_dict = {}\n",
    "    for index, language in enumerate(set(y)):\n",
    "        lang_dict[language] = index\n",
    "    y = list(map(lambda x: lang_dict[x],y))\n",
    "    return keras.utils.to_categorical(y, len(lang_dict)), lang_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load native speakers function\n",
    "def load_native_speakers(path: str) -> list[str]:\n",
    "    file_paths = [file for file in os.listdir(path) if file.endswith('.wav')]\n",
    "    return file_paths\n",
    "\n",
    "# combine native speakers function, take file paths, number of samples\n",
    "def combine_native_speakers(\n",
    "    file_path: str, \n",
    "    paths:list[str],\n",
    "    n_samples: int = 2, \n",
    "    seed_duration:int = 1,\n",
    ") -> list[np.array]:\n",
    "    # load the primary audio\n",
    "    primary_audio, sample_rate = librosa.load(f'./data/audio/{file_path}', sr=None)\n",
    "    # get the primary one second seed\n",
    "    primary_one_seed = librosa.util.fix_length(primary_audio, size=seed_duration)\n",
    "    # randonly select the secondary audio in paths\n",
    "    secondary_audio, _ = librosa.load(f'./data/audio/{random.choice(paths)}')\n",
    "    # random 2 second seed\n",
    "    secondary_one_seed = librosa.util.fix_length(secondary_audio, size=n_samples)\n",
    "    \n",
    "    # combine the primary and secondary audio\n",
    "    combined_audio = np.concatenate((primary_one_seed, secondary_one_seed))\n",
    "    return combined_audio, sample_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_people(\n",
    "    dataframe: pd.DataFrame, \n",
    "    test_size: float = 0.2, \n",
    "    first_column: str = 'language_num',\n",
    "    second_column: str = 'english_residence',\n",
    ") -> any:\n",
    "    \"\"\"\n",
    "        Create train test split of DataFrame\n",
    "        Args:\n",
    "            :param dataframe: DataFrame to be split\n",
    "            :param test_size: Percentage of total files to be split into test\n",
    "        Return:\n",
    "            :return X_train, X_test, y_train, y_test (tuple): Xs are list of\n",
    "            df['language_num'] and Ys are df['english_residence']\n",
    "    \"\"\"\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        dataframe[first_column],\n",
    "        dataframe[second_column],\n",
    "        test_size=test_size,\n",
    "        train_size= 1 - test_size,\n",
    "        random_state=1234\n",
    "    )\n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>href</th>\n",
       "      <th>language_num</th>\n",
       "      <th>sex</th>\n",
       "      <th>birth_place</th>\n",
       "      <th>native_language</th>\n",
       "      <th>other_languages</th>\n",
       "      <th>age_sex</th>\n",
       "      <th>age_of_english_onset</th>\n",
       "      <th>english_learning_method</th>\n",
       "      <th>english_residence</th>\n",
       "      <th>length_of_english_residence</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://accent.gmu.edu/browse_language.php?func...</td>\n",
       "      <td>mandarin1</td>\n",
       "      <td>female</td>\n",
       "      <td>['shanxi,', 'china']</td>\n",
       "      <td>mandarin\\n(cmn)</td>\n",
       "      <td>['none']</td>\n",
       "      <td>['26,', 'female', '']</td>\n",
       "      <td>13.0</td>\n",
       "      <td>academic</td>\n",
       "      <td>usa</td>\n",
       "      <td>2.0</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://accent.gmu.edu/browse_language.php?func...</td>\n",
       "      <td>mandarin2</td>\n",
       "      <td>female</td>\n",
       "      <td>['nanjing,', 'china']</td>\n",
       "      <td>mandarin\\n(cmn)</td>\n",
       "      <td>['japanese', '']</td>\n",
       "      <td>['38,', 'female', '']</td>\n",
       "      <td>14.0</td>\n",
       "      <td>academic</td>\n",
       "      <td>usa</td>\n",
       "      <td>0.8</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://accent.gmu.edu/browse_language.php?func...</td>\n",
       "      <td>mandarin3</td>\n",
       "      <td>male</td>\n",
       "      <td>['jilin,', 'china']</td>\n",
       "      <td>mandarin\\n(cmn)</td>\n",
       "      <td>['italian', 'german', 'french', '']</td>\n",
       "      <td>['43,', 'male', '']</td>\n",
       "      <td>10.0</td>\n",
       "      <td>academic</td>\n",
       "      <td>usa</td>\n",
       "      <td>14.0</td>\n",
       "      <td>43.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://accent.gmu.edu/browse_language.php?func...</td>\n",
       "      <td>mandarin4</td>\n",
       "      <td>female</td>\n",
       "      <td>['shanghai,', 'china']</td>\n",
       "      <td>mandarin\\n(cmn)</td>\n",
       "      <td>['japanese', '']</td>\n",
       "      <td>['24,', 'female', '']</td>\n",
       "      <td>6.0</td>\n",
       "      <td>academic</td>\n",
       "      <td>usa</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://accent.gmu.edu/browse_language.php?func...</td>\n",
       "      <td>mandarin5</td>\n",
       "      <td>female</td>\n",
       "      <td>['beijing,', 'china']</td>\n",
       "      <td>mandarin\\n(cmn)</td>\n",
       "      <td>['none']</td>\n",
       "      <td>['31,', 'female', '']</td>\n",
       "      <td>12.0</td>\n",
       "      <td>academic</td>\n",
       "      <td>usa</td>\n",
       "      <td>2.0</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                href language_num     sex  \\\n",
       "0  http://accent.gmu.edu/browse_language.php?func...    mandarin1  female   \n",
       "1  http://accent.gmu.edu/browse_language.php?func...    mandarin2  female   \n",
       "2  http://accent.gmu.edu/browse_language.php?func...    mandarin3    male   \n",
       "3  http://accent.gmu.edu/browse_language.php?func...    mandarin4  female   \n",
       "4  http://accent.gmu.edu/browse_language.php?func...    mandarin5  female   \n",
       "\n",
       "              birth_place  native_language  \\\n",
       "0    ['shanxi,', 'china']  mandarin\\n(cmn)   \n",
       "1   ['nanjing,', 'china']  mandarin\\n(cmn)   \n",
       "2     ['jilin,', 'china']  mandarin\\n(cmn)   \n",
       "3  ['shanghai,', 'china']  mandarin\\n(cmn)   \n",
       "4   ['beijing,', 'china']  mandarin\\n(cmn)   \n",
       "\n",
       "                       other_languages                age_sex  \\\n",
       "0                             ['none']  ['26,', 'female', '']   \n",
       "1                     ['japanese', '']  ['38,', 'female', '']   \n",
       "2  ['italian', 'german', 'french', '']    ['43,', 'male', '']   \n",
       "3                     ['japanese', '']  ['24,', 'female', '']   \n",
       "4                             ['none']  ['31,', 'female', '']   \n",
       "\n",
       "   age_of_english_onset english_learning_method english_residence  \\\n",
       "0                  13.0                academic               usa   \n",
       "1                  14.0                academic               usa   \n",
       "2                  10.0                academic               usa   \n",
       "3                   6.0                academic               usa   \n",
       "4                  12.0                academic               usa   \n",
       "\n",
       "   length_of_english_residence   age  \n",
       "0                          2.0  26.0  \n",
       "1                          0.8  38.0  \n",
       "2                         14.0  43.0  \n",
       "3                          1.0  24.0  \n",
       "4                          2.0  31.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the native_bio_metadata.csv\n",
    "native_bio_metadata = pd.read_csv(NATIVE_FILE_PATH)\n",
    "native_bio_metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # audio feature extraction instance\n",
    "# load native speaker files\n",
    "native_speakers_data = load_native_speakers(path=NATIVE_DIR)\n",
    "# native_speakers_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine native speakers\n",
    "native_speakers = []\n",
    "for file in native_speakers_data:\n",
    "    aud, sr = combine_native_speakers(file, native_speakers_data)\n",
    "    fwx = file.split('.')[0]\n",
    "    # get the english residence from the native_bio_metadata['english_residence'] based on the filename\n",
    "    class_category = native_bio_metadata[native_bio_metadata['language_num'] == fwx]['english_residence'].values[0]\n",
    "    # create three columns, audio, sample rate, and file name\n",
    "    native_speakers.append([file, class_category])\n",
    "    os.makedirs(NATIVE_COMBINED_DIR, exist_ok=True)\n",
    "    sf.write(f'./data/native_combined/{file}', aud, sr, subtype='PCM_24')\n",
    "    \n",
    "# create a dataframe from the native speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>english_residence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>arabic1.wav</td>\n",
       "      <td>usa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>arabic10.wav</td>\n",
       "      <td>usa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>arabic100.wav</td>\n",
       "      <td>usa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>arabic101.wav</td>\n",
       "      <td>usa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>arabic102.wav</td>\n",
       "      <td>usa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       file_name english_residence\n",
       "0    arabic1.wav               usa\n",
       "1   arabic10.wav               usa\n",
       "2  arabic100.wav               usa\n",
       "3  arabic101.wav               usa\n",
       "4  arabic102.wav               usa"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a dataframe from the native speakers\n",
    "df = pd.DataFrame(\n",
    "    native_speakers, \n",
    "    columns=[\n",
    "        'file_name',\n",
    "        'english_residence',\n",
    "    ]\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>english_residence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1169</td>\n",
       "      <td>1169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>1169</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>arabic1.wav</td>\n",
       "      <td>usa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>1031</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          file_name english_residence\n",
       "count          1169              1169\n",
       "unique         1169                 3\n",
       "top     arabic1.wav               usa\n",
       "freq              1              1031"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "english_residence\n",
       "usa       1031\n",
       "uk          81\n",
       "canada      57\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['english_residence'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the native_combined_df into train and test\n",
    "X_train, X_test, y_train, y_test = split_people(\n",
    "    df, \n",
    "    first_column='file_name', \n",
    "    second_column='english_residence', \n",
    "    test_size=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(234,) (234,)\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test.apply(lambda x: x.split('\\n')[0])\n",
    "y_train = y_train.apply(lambda x: x.split('\\n')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Count: Counter({'usa': 832, 'uk': 64, 'canada': 39})\n",
      "Test Count: Counter({'usa': 199, 'canada': 18, 'uk': 17})\n"
     ]
    }
   ],
   "source": [
    "# Get statistics\n",
    "train_count = Counter(y_train)\n",
    "test_count = Counter(y_test)\n",
    "\n",
    "\n",
    "print(f\"Train Count: {train_count}\")\n",
    "print(f\"Test Count: {test_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_cat, _ = to_categorical(y_train)\n",
    "y_test_cat, lang_dict = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'usa': 0, 'canada': 1, 'uk': 2}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arabic1.wav'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\musah\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\librosa\\core\\spectrum.py:257: UserWarning: n_fft=2048 is too large for input signal of length=2\n",
      "  warnings.warn(\n",
      "c:\\Users\\musah\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\librosa\\core\\pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n",
      "  return pitch_tuning(\n",
      "c:\\Users\\musah\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\librosa\\core\\spectrum.py:257: UserWarning: n_fft=2048 is too large for input signal of length=3\n",
      "  warnings.warn(\n",
      "c:\\Users\\musah\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\librosa\\core\\spectrum.py:257: UserWarning: n_fft=2048 is too large for input signal of length=5\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# extract acoustic features\n",
    "X_train_acoustic = []\n",
    "\n",
    "for audio in X_train:\n",
    "    # audio is a waveform, load it and extract the features\n",
    "    # mfcc, chroma_stft, spectral_centroid, spectral_bandwidth, spectral_rolloff, zero_crossing_rate\n",
    "    mfcc, chroma_stft, _, _, _, _ = extract_accoustic_features(audio)\n",
    "    \n",
    "    X_train_acoustic.append([\n",
    "        mfcc, \n",
    "        chroma_stft,\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-7.3805859e+02, -7.8013401e+00,  3.7200711e+00, -1.6094973e+00,\n",
       "         6.4021707e-01, -3.1855926e-01,  1.4145039e-01, -9.8202810e-02,\n",
       "         4.3154776e-02, -4.5751110e-02], dtype=float32),\n",
       " array([0.9135525 , 0.94166386, 0.96907395, 0.99474573, 1.        ,\n",
       "        0.8222989 , 0.74025804, 0.76661295, 0.7962481 , 0.8263944 ,\n",
       "        0.8554899 , 0.8836181 ], dtype=float32)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_acoustic[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_acoustic = []\n",
    "\n",
    "for file in X_test:\n",
    "    audio = file\n",
    "    mfcc, chroma_stft, _, _, _, _ = extract_accoustic_features(audio)\n",
    "    X_test_acoustic.append([\n",
    "        mfcc,\n",
    "        chroma_stft,\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-7.3805859e+02, -7.8013401e+00,  3.7200711e+00, -1.6094973e+00,\n",
       "         6.4021707e-01, -3.1855926e-01,  1.4145039e-01, -9.8202810e-02,\n",
       "         4.3154776e-02, -4.5751110e-02], dtype=float32),\n",
       " array([0.9135525 , 0.94166386, 0.96907395, 0.99474573, 1.        ,\n",
       "        0.8222989 , 0.74025804, 0.76661295, 0.7962481 , 0.8263944 ,\n",
       "        0.8554899 , 0.8836181 ], dtype=float32)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_acoustic[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pytorch model to train the data on\n",
    "class AcousticDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Concatenate the features within each sample\n",
    "        features = torch.tensor(np.concatenate(self.X[idx], axis=0), dtype=torch.float32)\n",
    "        label = torch.tensor(self.y[idx], dtype=torch.long)  # Assuming self.y[idx] is the label index\n",
    "        return features, label\n",
    "\n",
    "class AcousticModelSequntial(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(AcousticModelSequntial, self).__init__()\n",
    "        # self.model = nn.Sequential(\n",
    "        #     nn.Linear(input_size, 32),\n",
    "        #     nn.Conv1d(32, 64, 3),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Linear(64, output_size),\n",
    "        #     nn.Softmax(dim=1),\n",
    "        #     nn.Flatten()\n",
    "        # )\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, output_size),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# create the model\n",
    "class AcousticModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(AcousticModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AcousticModel(\n",
       "  (fc1): Linear(in_features=22, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=3, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the dimension of input features\n",
    "input_size = X_train_acoustic[0][0].shape[0] + X_train_acoustic[0][1].shape[0]  # + X_train_acoustic[0][2].shape[0] + X_train_acoustic[0][n].shape[0]\n",
    "\n",
    "# model_1 = AcousticModelSequntial(input_size=input_size, output_size=len(lang_dict))\n",
    "# model_1.to(device)\n",
    "# optimizer_1 = optim.Adam(model_1.parameters(), lr=LEARNING_RATE)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# model_1.train()\n",
    "\n",
    "\n",
    "# create the model\n",
    "model = AcousticModel(input_size=input_size, output_size=len(lang_dict))\n",
    "model.to(device)\n",
    "# create the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# create the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# create the data loaders\n",
    "train_dataset = AcousticDataset(X_train_acoustic, y_train_cat)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "# set the model to training mode\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = AcousticDataset(X_train_acoustic, y_train_cat)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the data loaders\n",
    "val_dataset = AcousticDataset(X_test_acoustic, y_test_cat)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # fit the model\n",
    "# for epoch in range(EPOCHS):\n",
    "#     for i, (X, y) in enumerate(train_loader):\n",
    "#         X = X.clone().detach().to(device)  # Use clone().detach() to copy and detach the tensor\n",
    "#         y = torch.argmax(y.clone().detach(), dim=1).to(device)  # Similarly, clone().detach() for y\n",
    "\n",
    "#         optimizer_1.zero_grad()\n",
    "#         outputs = model_1(X)\n",
    "#         loss = criterion(outputs, y)\n",
    "#         loss.backward()\n",
    "        \n",
    "#         optimizer_1.step()\n",
    "        \n",
    "#         if i % 10 == 0:\n",
    "#             print(f\"Epoch: {epoch} Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 0.645207405090332\n",
      "Epoch: 0 Loss: 0.6139446496963501\n",
      "Epoch: 0 Loss: 0.6451946496963501\n",
      "Epoch: 1 Loss: 0.6139446496963501\n",
      "Epoch: 1 Loss: 0.6764446496963501\n",
      "Epoch: 1 Loss: 0.7076947689056396\n",
      "Epoch: 2 Loss: 0.6764446496963501\n",
      "Epoch: 2 Loss: 0.6139446496963501\n",
      "Epoch: 2 Loss: 0.6764446496963501\n",
      "Epoch: 3 Loss: 0.7076947093009949\n",
      "Epoch: 3 Loss: 0.7389447093009949\n",
      "Epoch: 3 Loss: 0.7389446496963501\n",
      "Epoch: 4 Loss: 0.6764446496963501\n",
      "Epoch: 4 Loss: 0.6451946496963501\n",
      "Epoch: 4 Loss: 0.5826946496963501\n",
      "Epoch: 5 Loss: 0.6139446496963501\n",
      "Epoch: 5 Loss: 0.6451946496963501\n",
      "Epoch: 5 Loss: 0.7076947093009949\n",
      "Epoch: 6 Loss: 0.6139446496963501\n",
      "Epoch: 6 Loss: 0.6764446496963501\n",
      "Epoch: 6 Loss: 0.6139446496963501\n",
      "Epoch: 7 Loss: 0.6451946496963501\n",
      "Epoch: 7 Loss: 0.6139446496963501\n",
      "Epoch: 7 Loss: 0.7389446496963501\n",
      "Epoch: 8 Loss: 0.6451946496963501\n",
      "Epoch: 8 Loss: 0.6764446496963501\n",
      "Epoch: 8 Loss: 0.7389447093009949\n",
      "Epoch: 9 Loss: 0.8014447689056396\n",
      "Epoch: 9 Loss: 0.6764446496963501\n",
      "Epoch: 9 Loss: 0.6764446496963501\n",
      "Epoch: 10 Loss: 0.6764446496963501\n",
      "Epoch: 10 Loss: 0.5826946496963501\n",
      "Epoch: 10 Loss: 0.6139446496963501\n",
      "Epoch: 11 Loss: 0.6451946496963501\n",
      "Epoch: 11 Loss: 0.7389446496963501\n",
      "Epoch: 11 Loss: 0.6451946496963501\n",
      "Epoch: 12 Loss: 0.7076947093009949\n",
      "Epoch: 12 Loss: 0.6139446496963501\n",
      "Epoch: 12 Loss: 0.7076946496963501\n",
      "Epoch: 13 Loss: 0.6139446496963501\n",
      "Epoch: 13 Loss: 0.5826946496963501\n",
      "Epoch: 13 Loss: 0.6451946496963501\n",
      "Epoch: 14 Loss: 0.7389446496963501\n",
      "Epoch: 14 Loss: 0.7389447093009949\n",
      "Epoch: 14 Loss: 0.6451946496963501\n",
      "Epoch: 15 Loss: 0.6764447093009949\n",
      "Epoch: 15 Loss: 0.6451946496963501\n",
      "Epoch: 15 Loss: 0.6451946496963501\n",
      "Epoch: 16 Loss: 0.6451946496963501\n",
      "Epoch: 16 Loss: 0.5826946496963501\n",
      "Epoch: 16 Loss: 0.7076946496963501\n",
      "Epoch: 17 Loss: 0.7076946496963501\n",
      "Epoch: 17 Loss: 0.7076947689056396\n",
      "Epoch: 17 Loss: 0.6764446496963501\n",
      "Epoch: 18 Loss: 0.6451946496963501\n",
      "Epoch: 18 Loss: 0.7076947093009949\n",
      "Epoch: 18 Loss: 0.6764447093009949\n",
      "Epoch: 19 Loss: 0.6764446496963501\n",
      "Epoch: 19 Loss: 0.6451946496963501\n",
      "Epoch: 19 Loss: 0.5826946496963501\n",
      "Epoch: 20 Loss: 0.6139446496963501\n",
      "Epoch: 20 Loss: 0.6451946496963501\n",
      "Epoch: 20 Loss: 0.6451946496963501\n",
      "Epoch: 21 Loss: 0.7076946496963501\n",
      "Epoch: 21 Loss: 0.6451946496963501\n",
      "Epoch: 21 Loss: 0.5826946496963501\n",
      "Epoch: 22 Loss: 0.6764446496963501\n",
      "Epoch: 22 Loss: 0.6139446496963501\n",
      "Epoch: 22 Loss: 0.6764447093009949\n",
      "Epoch: 23 Loss: 0.6139446496963501\n",
      "Epoch: 23 Loss: 0.7076947093009949\n",
      "Epoch: 23 Loss: 0.6764446496963501\n",
      "Epoch: 24 Loss: 0.6139446496963501\n",
      "Epoch: 24 Loss: 0.7076946496963501\n",
      "Epoch: 24 Loss: 0.7389447093009949\n",
      "Epoch: 25 Loss: 0.6139446496963501\n",
      "Epoch: 25 Loss: 0.5514445900917053\n",
      "Epoch: 25 Loss: 0.6451946496963501\n",
      "Epoch: 26 Loss: 0.6451946496963501\n",
      "Epoch: 26 Loss: 0.6139446496963501\n",
      "Epoch: 26 Loss: 0.6451946496963501\n",
      "Epoch: 27 Loss: 0.6451946496963501\n",
      "Epoch: 27 Loss: 0.6451946496963501\n",
      "Epoch: 27 Loss: 0.6764447093009949\n",
      "Epoch: 28 Loss: 0.7076947093009949\n",
      "Epoch: 28 Loss: 0.6764446496963501\n",
      "Epoch: 28 Loss: 0.7076947093009949\n",
      "Epoch: 29 Loss: 0.7076947093009949\n",
      "Epoch: 29 Loss: 0.5514445900917053\n",
      "Epoch: 29 Loss: 0.5826946496963501\n",
      "Epoch: 30 Loss: 0.8326947689056396\n",
      "Epoch: 30 Loss: 0.6764447093009949\n",
      "Epoch: 30 Loss: 0.6451946496963501\n",
      "Epoch: 31 Loss: 0.5826946496963501\n",
      "Epoch: 31 Loss: 0.7076947093009949\n",
      "Epoch: 31 Loss: 0.6451946496963501\n",
      "Epoch: 32 Loss: 0.6764446496963501\n",
      "Epoch: 32 Loss: 0.6451946496963501\n",
      "Epoch: 32 Loss: 0.6764446496963501\n",
      "Epoch: 33 Loss: 0.6764446496963501\n",
      "Epoch: 33 Loss: 0.6764447093009949\n",
      "Epoch: 33 Loss: 0.6451946496963501\n",
      "Epoch: 34 Loss: 0.6451946496963501\n",
      "Epoch: 34 Loss: 0.6451946496963501\n",
      "Epoch: 34 Loss: 0.6139446496963501\n",
      "Epoch: 35 Loss: 0.6451946496963501\n",
      "Epoch: 35 Loss: 0.6451946496963501\n",
      "Epoch: 35 Loss: 0.6451946496963501\n",
      "Epoch: 36 Loss: 0.5826946496963501\n",
      "Epoch: 36 Loss: 0.7701947689056396\n",
      "Epoch: 36 Loss: 0.6764446496963501\n",
      "Epoch: 37 Loss: 0.6451946496963501\n",
      "Epoch: 37 Loss: 0.6139446496963501\n",
      "Epoch: 37 Loss: 0.7076947093009949\n",
      "Epoch: 38 Loss: 0.6764446496963501\n",
      "Epoch: 38 Loss: 0.6139446496963501\n",
      "Epoch: 38 Loss: 0.6139446496963501\n",
      "Epoch: 39 Loss: 0.7389447689056396\n",
      "Epoch: 39 Loss: 0.6451946496963501\n",
      "Epoch: 39 Loss: 0.6764447093009949\n",
      "Epoch: 40 Loss: 0.6451946496963501\n",
      "Epoch: 40 Loss: 0.6451946496963501\n",
      "Epoch: 40 Loss: 0.6139446496963501\n",
      "Epoch: 41 Loss: 0.7076947093009949\n",
      "Epoch: 41 Loss: 0.7389447093009949\n",
      "Epoch: 41 Loss: 0.7701947093009949\n",
      "Epoch: 42 Loss: 0.6451946496963501\n",
      "Epoch: 42 Loss: 0.5826946496963501\n",
      "Epoch: 42 Loss: 0.6451946496963501\n",
      "Epoch: 43 Loss: 0.6139446496963501\n",
      "Epoch: 43 Loss: 0.6139446496963501\n",
      "Epoch: 43 Loss: 0.6139446496963501\n",
      "Epoch: 44 Loss: 0.7389446496963501\n",
      "Epoch: 44 Loss: 0.7701947093009949\n",
      "Epoch: 44 Loss: 0.6451946496963501\n",
      "Epoch: 45 Loss: 0.7389446496963501\n",
      "Epoch: 45 Loss: 0.7076947093009949\n",
      "Epoch: 45 Loss: 0.6451946496963501\n",
      "Epoch: 46 Loss: 0.5826946496963501\n",
      "Epoch: 46 Loss: 0.6451946496963501\n",
      "Epoch: 46 Loss: 0.7701947093009949\n",
      "Epoch: 47 Loss: 0.6139446496963501\n",
      "Epoch: 47 Loss: 0.6451946496963501\n",
      "Epoch: 47 Loss: 0.6451946496963501\n",
      "Epoch: 48 Loss: 0.6764447093009949\n",
      "Epoch: 48 Loss: 0.7389446496963501\n",
      "Epoch: 48 Loss: 0.6139446496963501\n",
      "Epoch: 49 Loss: 0.6139446496963501\n",
      "Epoch: 49 Loss: 0.6764446496963501\n",
      "Epoch: 49 Loss: 0.6451946496963501\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "for epoch in range(EPOCHS):\n",
    "    for i, (X, y) in enumerate(train_loader):\n",
    "        X = X.clone().detach().to(device)  # Use clone().detach() to copy and detach the tensor\n",
    "        y = torch.argmax(y.clone().detach(), dim=1).to(device)  # Similarly, clone().detach() for y\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print(f\"Epoch: {epoch} Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a validation or test DataLoader called val_loader\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_val, y_val in val_loader:\n",
    "        X_val = X_val.to(device)\n",
    "        y_val = y_val.to(device)\n",
    "\n",
    "        outputs = model(X_val)\n",
    "        _, predicted = torch.max(outputs, 1)  # Get the index of the max logit as prediction\n",
    "        print(y_val.shape, predicted.shape)\n",
    "        print(predicted)\n",
    "        # correct += (predicted == y_val).sum().item()\n",
    "        # total += y_val.size(0)\n",
    "\n",
    "# accuracy = correct / total\n",
    "# print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
